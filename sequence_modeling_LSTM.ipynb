{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File Path: /home/jupyter/ijahan1/textProcessing/sample_data/\n",
      "\n",
      "DataFrame: \n",
      "                    timestamp event_value    sensor_value\n",
      "0  2019-04-04 00:00:00.000049       camon    cam_on_obsrv\n",
      "1  2019-04-04 00:00:14.747171       fanon    fan_on_obsrv\n",
      "2  2019-04-04 00:00:15.031492        pcon  pc_on_no_obsrv\n",
      "3  2019-04-04 00:00:15.139317       scron    scr_on_obsrv\n",
      "4  2019-04-04 00:02:12.125632      fanoff   fan_off_obsrv\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 19, 128)           2944      \n",
      "_________________________________________________________________\n",
      "bidirectional_38 (Bidirectio (None, 19, 256)           263168    \n",
      "_________________________________________________________________\n",
      "bidirectional_39 (Bidirectio (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 23)                2967      \n",
      "=================================================================\n",
      "Total params: 433,431\n",
      "Trainable params: 433,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training Time!\n",
      "Epoch 1/100\n",
      "14/14 [==============================] - 5s 18ms/step - loss: 3.0620 - accuracy: 0.0943\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 2.7877 - accuracy: 0.1533\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.6326 - accuracy: 0.1389\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.5634 - accuracy: 0.2317\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.4072 - accuracy: 0.2373\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.3679 - accuracy: 0.2625\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 2.3942 - accuracy: 0.2357\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.2489 - accuracy: 0.3022\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.2395 - accuracy: 0.2772\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.2356 - accuracy: 0.2800\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.1674 - accuracy: 0.3217\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.2156 - accuracy: 0.3371\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.1164 - accuracy: 0.3654\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 2.0895 - accuracy: 0.3712\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 2.0262 - accuracy: 0.4351\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 1.9666 - accuracy: 0.4187\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.8805 - accuracy: 0.4412\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.8462 - accuracy: 0.4373\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.6839 - accuracy: 0.5187\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.7020 - accuracy: 0.4974\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.6237 - accuracy: 0.5247\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.5399 - accuracy: 0.5610\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.6008 - accuracy: 0.5311\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.5445 - accuracy: 0.5129\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.4413 - accuracy: 0.5747\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.4424 - accuracy: 0.5662\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.2792 - accuracy: 0.6192\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.3617 - accuracy: 0.5754\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.3237 - accuracy: 0.5926\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.2966 - accuracy: 0.5833\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.2410 - accuracy: 0.6115\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.1674 - accuracy: 0.6246\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.1266 - accuracy: 0.6572\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.0769 - accuracy: 0.6657\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.1332 - accuracy: 0.6252\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.0518 - accuracy: 0.6583\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.1510 - accuracy: 0.6231\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 1.1010 - accuracy: 0.6674\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.0006 - accuracy: 0.7020\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.9496 - accuracy: 0.7076\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.9306 - accuracy: 0.7045\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.9683 - accuracy: 0.6818\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.9380 - accuracy: 0.7029\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.9043 - accuracy: 0.7051\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.8173 - accuracy: 0.7594\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.7817 - accuracy: 0.7515\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.7698 - accuracy: 0.7604\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.8396 - accuracy: 0.7201\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.7806 - accuracy: 0.7440\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.7381 - accuracy: 0.7639\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.6642 - accuracy: 0.7962\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.6434 - accuracy: 0.7942\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.6858 - accuracy: 0.7768\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.6142 - accuracy: 0.8125\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.6274 - accuracy: 0.8033\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.6120 - accuracy: 0.8147\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.6210 - accuracy: 0.7983\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.5126 - accuracy: 0.8443\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.5295 - accuracy: 0.8378\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.5038 - accuracy: 0.8384\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4930 - accuracy: 0.8574\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4509 - accuracy: 0.8679\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4387 - accuracy: 0.8835\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4296 - accuracy: 0.8694\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.5323 - accuracy: 0.8336\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5529 - accuracy: 0.8313\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.5452 - accuracy: 0.8168\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.5351 - accuracy: 0.8175\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4648 - accuracy: 0.8515\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3939 - accuracy: 0.8961\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3886 - accuracy: 0.8990\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3716 - accuracy: 0.8749\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3485 - accuracy: 0.9149\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3741 - accuracy: 0.8938\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4923 - accuracy: 0.8503\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.7941 - accuracy: 0.7560\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5834 - accuracy: 0.8060\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4971 - accuracy: 0.8536\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4434 - accuracy: 0.8614\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3713 - accuracy: 0.8855\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3194 - accuracy: 0.9028\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3258 - accuracy: 0.9116\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3168 - accuracy: 0.8936\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3213 - accuracy: 0.9153\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3055 - accuracy: 0.9026\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.2715 - accuracy: 0.9258\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.2786 - accuracy: 0.9174\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.2531 - accuracy: 0.9311\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.2352 - accuracy: 0.9381\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.2125 - accuracy: 0.9373\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.1700 - accuracy: 0.9623\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.1731 - accuracy: 0.9525\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.1827 - accuracy: 0.9397\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.1610 - accuracy: 0.9534\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.1521 - accuracy: 0.9546\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.1726 - accuracy: 0.9311\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.1668 - accuracy: 0.9346\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.1434 - accuracy: 0.9599\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.1403 - accuracy: 0.9615\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.1550 - accuracy: 0.9588\n",
      "\n",
      "Predicted list for \"dooron\" is: \n",
      "['dooron', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'camoff', 'fanoff', 'fanoff', 'fanoff', 'fanoff', 'fanoff', 'fridgeoff', 'fridgeoff', 'camon', 'fridgeoff', 'coffee3', 'radon', 'fridgeon', 'fridgeon', 'fridgeon', 'fridgeoff', 'coffee3', 'coffee3', 'coffee3', 'coffee3', 'coffee3', 'coffee3', 'lightoff', 'lightoff', 'lightoff']\n",
      "\n",
      "Predicted list for \"dooron\" (uniqe) is: \n",
      "['dooron', 'camoff', 'fanoff', 'fridgeoff', 'camon', 'coffee3', 'radon', 'fridgeon', 'lightoff']\n"
     ]
    }
   ],
   "source": [
    "## Import the neccessary libraries and packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ***************************************************************************************************************\n",
    "# ***************************************************************************************************************\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "def read_data(file_path):\n",
    "    dfs = pd.DataFrame()\n",
    "\n",
    "    # Retrieve all CSV file from 'sample_data' folder\n",
    "    file_names = glob.glob(file_path + '*.csv')\n",
    "\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(file_name)\n",
    "        dfs = dfs.append(df, ignore_index = True)\n",
    "        del df\n",
    "\n",
    "    # Remove the '_' (uderscore) with from 'event_value' column\n",
    "    dfs['event_value'] = dfs['event_value'].str.replace('_', '')\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "# ****************************************************************************************************************\n",
    "# ****************************************************************************************************************\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    # Data Preprocessing\n",
    "    corpus_ = df['event_value'].values.flatten()\n",
    "\n",
    "    corpus = []\n",
    "    sentence_size = 20\n",
    "    count = 0\n",
    "    temp_sentence = ''\n",
    "\n",
    "    for item in corpus_:\n",
    "        count += 1\n",
    "        if count < sentence_size:\n",
    "            temp_sentence += item + ' '\n",
    "        else:\n",
    "            temp_sentence += item\n",
    "            corpus.append(temp_sentence)\n",
    "            temp_sentence = ''\n",
    "            count = 0\n",
    "            \n",
    "    # Create tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    # Define the total words. Add 1 for the index `0` which is just the padding token.\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # Initialize the sequences list\n",
    "    input_sequences = []\n",
    "\n",
    "    # Loop over the line several times to generate the subphrases\n",
    "    for line in corpus:\n",
    "        # Tokenize the current line\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "        for i in range(1, len(token_list)):\n",
    "\n",
    "            # Generate the subphrase\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "            # Append the subphrase to the sequences list\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "            \n",
    "    \n",
    "    # Get the length of the longest line\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "    # Pad all sequences\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # Create inputs and label by splitting the last token in the subphrases\n",
    "    xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "    # Convert the label into one-hot arrays\n",
    "    ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "    \n",
    "    return xs, labels, ys, max_sequence_len, total_words\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************\n",
    "# ****************************************************************************************************************\n",
    "\n",
    "# Build the model\n",
    "def build_model(input_length, total_words):\n",
    "    model = Sequential([\n",
    "                Embedding(total_words, 128, input_length=max_sequence_len-1),\n",
    "                Bidirectional(LSTM(128, return_sequences=True)),\n",
    "                Bidirectional(LSTM(64)),\n",
    "                Dense(total_words, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Use categorical crossentropy because this is a multi-class problem\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Print the model summary\n",
    "    print('\\nModel Summary:')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************\n",
    "# ****************************************************************************************************************\n",
    "\n",
    "def predict_text(seed):\n",
    "    # Define seed text\n",
    "    seed_text = seed\n",
    "    prediction = [seed_text]\n",
    "\n",
    "    # Define total words to predict\n",
    "    next_words = 50\n",
    "\n",
    "    # Loop until desired length is reached\n",
    "    for _ in range(next_words):\n",
    "\n",
    "        # Convert the seed text to a token sequence\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        # Pad the sequence\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\n",
    "        # Feed to the model and get the probabilities for each index\n",
    "        probabilities = model.predict(token_list)\n",
    "\n",
    "        # Get the index with the highest probability\n",
    "        predicted = np.argmax(probabilities, axis=-1)[0]\n",
    "\n",
    "        # Ignore if index is 0 because that is just the padding.\n",
    "        if predicted != 0:\n",
    "\n",
    "            # Look up the word associated with the index. \n",
    "            output_word = tokenizer.index_word[predicted]\n",
    "\n",
    "            # Combine with the seed text\n",
    "            seed_text += \" \" + output_word\n",
    "            prediction.append(output_word)\n",
    "\n",
    "    prediction_unique = []\n",
    "    for item in prediction:\n",
    "        if item not in prediction_unique:\n",
    "            prediction_unique.append(item)\n",
    "    \n",
    "    return prediction, prediction_unique\n",
    "        \n",
    "# ****************************************************************************************************************\n",
    "# ****************************************************************************************************************\n",
    "\n",
    "def main():\n",
    "    # Read the data\n",
    "    file_path = os.path.join(os.getcwd(), 'sample_data/')\n",
    "    print(f'\\nFile Path: {file_path}')\n",
    "    df = read_data(file_path)\n",
    "    print(f'\\nDataFrame: \\n{df.head()}')\n",
    "    \n",
    "    xs, labels, ys, max_sequence_len, total_words = data_preprocessing(df)\n",
    "    \n",
    "    # get the model\n",
    "    model = build_model(max_sequence_len, total_words)\n",
    "    \n",
    "    # Train the model\n",
    "    print('\\nTraining Time!')\n",
    "    history = model.fit(xs, ys, epochs=100)\n",
    "    \n",
    "    seed = 'dooron'\n",
    "    \n",
    "    predicted_list, unique_predicted_list = predict_text(seed)\n",
    "    \n",
    "    print(f'\\nPredicted list for \"{seed}\" is: \\n{predicted_list}')\n",
    "    print(f'\\nPredicted list for \"{seed}\" (uniqe) is: \\n{unique_predicted_list}')\n",
    "\n",
    "# ****************************************************************************************************************\n",
    "# ****************************************************************************************************************\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
